{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "\n",
        "This notebook contains the exploratory data analysis for the AIAP 21 Technical Assessment.\n",
        "\n",
        "## Overview\n",
        "This notebook will analyze the gas monitoring data to understand patterns, trends, and insights.\n",
        "\n",
        "## Data Source\n",
        "- Database: `data/gas_monitoring.db`\n",
        "- Note: The database file should not be uploaded to the repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to the database\n",
        "db_path = 'data/gas_monitoring.db'\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "# Get table names\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "print(\"Available tables:\", [table[0] for table in tables])\n",
        "\n",
        "# TODO: Load and explore the data based on the actual table structure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Initial Exploration\n",
        "\n",
        "This section will load the data from the database and perform initial exploration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data from database\n",
        "db_path = 'data/gas_monitoring.db'\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "# Get table names\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "print(\"Available tables:\", [table[0] for table in tables])\n",
        "\n",
        "# Load data from each table\n",
        "dataframes = {}\n",
        "for table in tables:\n",
        "    table_name = table[0]\n",
        "    query = f\"SELECT * FROM {table_name}\"\n",
        "    df = pd.read_sql_query(query, conn)\n",
        "    dataframes[table_name] = df\n",
        "    print(f\"\\n{table_name} shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "# Display first few rows of each table\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"\\n=== {name.upper()} - First 5 rows ===\")\n",
        "    display(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data quality assessment\n",
        "print(\"ðŸ” DATA QUALITY ASSESSMENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"\\nðŸ“Š {name.upper()}\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    \n",
        "    print(\"\\nData Types:\")\n",
        "    print(df.dtypes)\n",
        "    \n",
        "    print(\"\\nMissing Values:\")\n",
        "    missing = df.isnull().sum()\n",
        "    if missing.sum() > 0:\n",
        "        print(missing[missing > 0])\n",
        "    else:\n",
        "        print(\"âœ… No missing values\")\n",
        "    \n",
        "    print(\"\\nDuplicate Rows:\")\n",
        "    duplicates = df.duplicated().sum()\n",
        "    print(f\"Found {duplicates} duplicate rows\")\n",
        "    \n",
        "    print(\"\\nUnique Values per Column:\")\n",
        "    for col in df.columns:\n",
        "        unique_count = df[col].nunique()\n",
        "        print(f\"  {col}: {unique_count} unique values\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical analysis\n",
        "print(\"ðŸ“ˆ STATISTICAL ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"\\nðŸ“Š {name.upper()}\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Descriptive statistics for numerical columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        print(\"Descriptive Statistics (Numerical Columns):\")\n",
        "        print(df[numeric_cols].describe())\n",
        "    \n",
        "    # Categorical analysis\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    if len(categorical_cols) > 0:\n",
        "        print(\"\\nCategorical Columns Analysis:\")\n",
        "        for col in categorical_cols:\n",
        "            print(f\"\\n{col}:\")\n",
        "            print(f\"  Unique values: {df[col].nunique()}\")\n",
        "            print(f\"  Most frequent: {df[col].value_counts().head(3).to_dict()}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization\n",
        "print(\"ðŸ“Š CREATING VISUALIZATIONS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Set up the plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create visualizations for each table\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"\\nðŸ“ˆ Creating visualizations for {name.upper()}\")\n",
        "    \n",
        "    # Get numerical columns for plotting\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    \n",
        "    if len(numeric_cols) > 0:\n",
        "        # Create subplots for numerical columns\n",
        "        n_cols = min(3, len(numeric_cols))\n",
        "        n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
        "        \n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "        if n_rows == 1:\n",
        "            axes = [axes] if n_cols == 1 else axes\n",
        "        else:\n",
        "            axes = axes.flatten()\n",
        "        \n",
        "        for i, col in enumerate(numeric_cols):\n",
        "            if i < len(axes):\n",
        "                # Histogram\n",
        "                axes[i].hist(df[col].dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
        "                axes[i].set_title(f'{col} Distribution')\n",
        "                axes[i].set_xlabel(col)\n",
        "                axes[i].set_ylabel('Frequency')\n",
        "        \n",
        "        # Hide unused subplots\n",
        "        for i in range(len(numeric_cols), len(axes)):\n",
        "            axes[i].set_visible(False)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Correlation heatmap if multiple numerical columns\n",
        "        if len(numeric_cols) > 1:\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            correlation_matrix = df[numeric_cols].corr()\n",
        "            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "            plt.title(f'{name.upper()} - Correlation Matrix')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    \n",
        "    # Categorical analysis\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    if len(categorical_cols) > 0:\n",
        "        for col in categorical_cols:\n",
        "            if df[col].nunique() <= 20:  # Only plot if reasonable number of categories\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                value_counts = df[col].value_counts()\n",
        "                value_counts.plot(kind='bar')\n",
        "                plt.title(f'{col} Value Counts')\n",
        "                plt.xlabel(col)\n",
        "                plt.ylabel('Count')\n",
        "                plt.xticks(rotation=45)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "print(\"\\nâœ… Visualizations completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Insights and Findings\n",
        "\n",
        "This section will summarize the key insights from the EDA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Key Insights and Findings\n",
        "print(\"ðŸ” KEY INSIGHTS AND FINDINGS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"\\nðŸ“Š {name.upper()}\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Data overview\n",
        "    print(f\"Dataset contains {df.shape[0]:,} rows and {df.shape[1]} columns\")\n",
        "    \n",
        "    # Missing data insights\n",
        "    missing_data = df.isnull().sum()\n",
        "    if missing_data.sum() > 0:\n",
        "        print(f\"âš ï¸  Missing data found in {missing_data[missing_data > 0].count()} columns\")\n",
        "        for col, missing_count in missing_data[missing_data > 0].items():\n",
        "            percentage = (missing_count / len(df)) * 100\n",
        "            print(f\"   - {col}: {missing_count:,} missing ({percentage:.1f}%)\")\n",
        "    else:\n",
        "        print(\"âœ… No missing data\")\n",
        "    \n",
        "    # Data quality insights\n",
        "    duplicates = df.duplicated().sum()\n",
        "    if duplicates > 0:\n",
        "        print(f\"âš ï¸  {duplicates:,} duplicate rows found\")\n",
        "    else:\n",
        "        print(\"âœ… No duplicate rows\")\n",
        "    \n",
        "    # Numerical insights\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        print(f\"\\nðŸ“ˆ Numerical Analysis ({len(numeric_cols)} columns):\")\n",
        "        for col in numeric_cols:\n",
        "            col_data = df[col].dropna()\n",
        "            if len(col_data) > 0:\n",
        "                print(f\"   - {col}:\")\n",
        "                print(f\"     Range: {col_data.min():.2f} to {col_data.max():.2f}\")\n",
        "                print(f\"     Mean: {col_data.mean():.2f}, Std: {col_data.std():.2f}\")\n",
        "                print(f\"     Skewness: {col_data.skew():.2f}\")\n",
        "    \n",
        "    # Categorical insights\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    if len(categorical_cols) > 0:\n",
        "        print(f\"\\nðŸ“‹ Categorical Analysis ({len(categorical_cols)} columns):\")\n",
        "        for col in categorical_cols:\n",
        "            unique_count = df[col].nunique()\n",
        "            print(f\"   - {col}: {unique_count} unique values\")\n",
        "            if unique_count <= 10:\n",
        "                top_values = df[col].value_counts().head(3)\n",
        "                print(f\"     Top values: {top_values.to_dict()}\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ RECOMMENDATIONS\")\n",
        "print(\"=\" * 50)\n",
        "print(\"1. Review missing data patterns and consider imputation strategies\")\n",
        "print(\"2. Investigate duplicate rows and determine if they should be removed\")\n",
        "print(\"3. Analyze numerical distributions for outliers and data quality issues\")\n",
        "print(\"4. Examine categorical variables for data consistency\")\n",
        "print(\"5. Consider feature engineering based on domain knowledge\")\n",
        "print(\"6. Plan further analysis based on business objectives\")\n",
        "\n",
        "# Close database connection\n",
        "conn.close()\n",
        "print(\"\\nðŸ”’ Database connection closed\")\n",
        "print(\"âœ… EDA Analysis Complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
